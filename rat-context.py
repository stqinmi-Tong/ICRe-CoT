import os
import pickle
import logging
from datetime import datetime
import numpy as np
import argparse
from datetime import datetime
from itertools import islice
import torch
from langchain.tools import Tool
import tiktoken
from multiprocessing import Process, Queue
import multiprocessing as mp
from difflib import unified_diff
import nltk
from collections import defaultdict
from openai import BadRequestError, OpenAIError, OpenAI
import openai
import json
import re
import logging
import faiss
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
import random
from prompt_selection import Demon_sampler
# ---- æ—¥å¿—é…ç½®----
log_filename = f"logs/run_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(log_filename),
        logging.StreamHandler()
    ]
)
# ensure punkt available once
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

class ChatGPT:
    def __init__(self, args, prompt_path, prompt_name, max_tokens):
        self.args = args
        self.history_messages = []
        self.history_contents = []
        self.max_tokens = max_tokens
        self.prompt = self.load_prompt_template(prompt_path, prompt_name)
        self.token_num = 0

    def get_response(self, input_text, turn_type):
        """
        è¿™æ˜¯ä¸»æ–¹æ³•ï¼Œç”¨äºŽå¤„ç†è¾“å…¥æ–‡æœ¬å¹¶èŽ·å– LLM çš„å“åº”ã€‚
        æ ¹æ®æ˜¯å¦ä¸º debug æ¨¡å¼å†³å®šï¼š
        âœ… è°ƒè¯•æ¨¡å¼ï¼ˆdebug = Trueï¼‰ï¼š
        åˆ›å»ºç”¨æˆ·æ¶ˆæ¯åŽæ˜¾ç¤ºæç¤ºï¼Œç­‰å¾…æ‰‹åŠ¨è¾“å…¥æ¨¡æ‹Ÿçš„è¿”å›žå€¼ã€‚
        ðŸš€ æ­£å¸¸è¿è¡Œæ¨¡å¼ï¼š
        è°ƒç”¨ query_API_to_get_message() å‘èµ·çœŸå®ž API è¯·æ±‚ã€‚
        å°†ç”¨æˆ·è¾“å…¥ä¸Žæ¨¡åž‹è¿”å›žéƒ½è¿½åŠ åˆ°åŽ†å²æ¶ˆæ¯ä¸­ã€‚
        """

        if self.args.debug:
            message = self.create_message(input_text, turn_type)
            self.history_messages.append(message)
            self.history_contents.append(message.content)
            print("query API to get message:\n%s" % message.content)
            response = input("input the returned response:")
        else:
            message = self.create_message(input_text, turn_type)
            self.history_messages.append(message)
            self.history_contents.append(message['content'])
            message = self.query_API_to_get_message(self.history_messages)
            # return res.choices[0].message
            self.history_messages.append(message)
            self.history_contents.append(message.content)
            response = message.content.strip()
        return response

    def create_message(self, input_text, turn_type):
        """
        æž„é€ ç”¨æˆ·æ¶ˆæ¯ï¼šcreate_message(input_text, turn_type)é€šè¿‡ä¸åŒçš„ turn_type æž„å»ºå¤šç§æç¤ºç±»åž‹çš„è¾“å…¥æ¶ˆæ¯ã€‚
          turn_type å†³å®šä½¿ç”¨çš„æ¨¡æ¿æ®µï¼Œå¦‚ï¼š
         "init_query": ä½¿ç”¨åˆå§‹åŒ–æŒ‡ä»¤ã€‚
         "first_give_demonstration": æä¾›åˆå§‹ç¤ºä¾‹ã€‚
         "analogy_demonstration" / "supplement_demonstration": ç±»æ¯”æˆ–è¡¥å……ç¤ºä¾‹ã€‚
         "final_query_template": ç”¨äºŽæœ€ç»ˆæé—®ï¼ŒåŒ…å«å€™é€‰å®žä½“åŠé—®é¢˜ã€‚
         "directly_ask": ç›´æŽ¥æé—®çš„æ¨¡æ¿ã€‚
        è¿”å›žå€¼ä¸ºï¼š{'role': 'user', 'content': input_text}
        """
        # res = self.LLM.get_response((question_text, answer), "query_generation")
        if turn_type == "first_give_demonstration":
            template = self.prompt['first_give_demonstration']
            question = input_text
            input_text = template.format(question=question)
        elif turn_type == "analogy_demonstration":
            template = self.prompt['analogy_demonstration']
            analogy_demons = input_text
            input_text = template.format(selected_analogy_demonstrations=analogy_demons)
        elif turn_type == "supplement_demonstration":
            template = self.prompt['supplement_demonstration']
            supplement_demons = input_text
            input_text = template.format(selected_supplement_demonstrations=supplement_demons)
        elif turn_type == "init_draft":
            template = self.prompt['init_draft']
            question, can_ents = input_text
            input_text = template.format(question=question, order_of_candidate=can_ents)
        elif turn_type == "query_generation":
            template = self.prompt['query_generation']
            question, answer = input_text
            input_text = template.format(question=question, answer=answer)
        elif turn_type == "revise":
            template = self.prompt['revise']
            question, answer, content = input_text
            input_text = template.format(content=content, question=question, answer=answer)
        elif turn_type == "final_query_template":
            template = self.prompt['final_query_template']
            origin_candidates_text,question_text,answer = input_text
            input_text = template.format(order_of_candidate=origin_candidates_text, question=question_text, CoT=answer)
        elif turn_type == "directly_ask":
            template = self.prompt['directly_ask']
            question = input_text
            input_text = template.format(question=question)
        else:
            raise NotImplementedError
        message = {'role': 'user', 'content': input_text}
        return message

    def count_tokens(self, messages, model="gpt-3.5-turbo"):
        """ç»Ÿè®¡ä¸€ç»„ messages çš„ token æ•°"""
        try:
            enc = tiktoken.encoding_for_model(model)
        except KeyError:
            enc = tiktoken.get_encoding("cl100k_base")  # fallback

        tokens = 0
        for msg in messages:
            tokens += 4  # role + metadata
            for key, value in msg.items():
                tokens += len(enc.encode(value))
        tokens += 2  # æ¯ä¸ª reply é¢å¤– tokens
        return tokens

    def query_API_to_get_message(self, messages):
        max_retries = 5
        retries = 0
        while True:
            try:
                client = OpenAI(api_key='sk-JMU1FWOBnZra7gVz1DRV8MlmNT7yCVr4VRB4PP539c5qRS80',
                                base_url='https://api.deepbricks.ai/v1/')
                #
                # tokens = self.count_tokens(messages, model="gpt-4o-mini")
                # print('message', messages)
                # print(f"[INFO] tokens={tokens}")

                res = client.chat.completions.create(
                    # model="o3-mini",
                    model="gpt-3.5-turbo",
                    # model="o4-mini",
                    # model="deepseek-R1-0528",
                    # model="GPT-5-chat",
                    messages=messages,
                    temperature=0,
                    max_tokens=self.max_tokens,
                    top_p=1,
                    frequency_penalty=0,
                    presence_penalty=0,
                )

                if self.args.debug_online:
                    print(res)

                self.token_num = res.usage.total_tokens
                return res.choices[0].message
            except Exception as e:
                print(f"OpenAI API error: {e}")

    def reset_history(self):
        self.history_messages = []
        self.history_contents = []
        self.token_num = 0

    def reset_history_messages(self):
        self.history_messages = []

    def reset_history_contents(self):
        self.history_contents = []

    def load_prompt_template(self, prompt_path, prompt_name):
        if prompt_path.endswith(".json"):
            with open(prompt_path, "rb") as f:
                prompt = json.load(f)
            return prompt[prompt_name]


class RAT:
    def __init__(self, args):
        # RAT Pipeline
        self.args = args
        self.LLM = ChatGPT(args=args, prompt_path=self.args.prompt_path, prompt_name=self.args.prompt_name,
                           max_tokens=self.args.max_tokens)
        self.newline_char = '\n'
        self.max_llm_input_token = self.args.max_llm_input_tokens
        self.prompt_selector = Demon_sampler(self.args)

        self.log = []
        self.candidate_answers = []
        self.selected_demonstrations = []

        self.id2ent = defaultdict(str)
        self.ent2id = defaultdict(str)
        self.rel2id = defaultdict(str)
        self.ent2text = defaultdict(str)
        self.all_candidate_answers = defaultdict(list)
        self.align_text = defaultdict(str)

        self.supporting_file_path = self.args.supporting_file_path
        self.chunk_size = self.args.chunk_size
        self.chunk_overlap = self.args.chunk_overlap
        # GPU åŠ é€Ÿ
        self.embedding_model = SentenceTransformer(
            '/root/miniconda3/envs/shent/models/msmarco-distilbert-base-v4',
            device="cuda" if torch.cuda.is_available() else "cpu"
        )

        # self.embedding_model = SentenceTransformer('/root/miniconda3/envs/shent/models/msmarco-distilbert-base-v4')
        self.encoding_name = "cl100k_base"

        self.chunks = self.load_and_chunk_text()
        # self.index, self.embeddings, self.chunks_search = self.build_faiss_index(self.chunks)
        # æž„å»ºæˆ–åŠ è½½ç¼“å­˜å¥½çš„ index
        self.index, self.chunks_search = self.build_faiss_index(
            self.chunks,
            save_path=f"cache/{self.args.dataset}_faiss"
        )
        # nltk.download('punkt')

        self.load_rel_txt_to_id()
        self.load_ent_map_id()
        self.load_all_candidate_answers()  ###å¾—åˆ°æ‰€æœ‰çš„å€™é€‰ä¸‰å…ƒç»„
        self.load_ent_to_text()
        if self.args.align_text:
            self.load_align_text()

    def load_and_chunk_text(self):
        """æŒ‰ chunk_size åŠ è½½æ–‡æœ¬"""
        with open(self.supporting_file_path, 'r') as f:
            text = f.read()

        step = self.chunk_size - self.chunk_overlap
        chunks = [text[i:i + self.chunk_size] for i in range(0, len(text), step)]
        return chunks

    def build_faiss_index(self, chunks, save_path="./outputs/faiss_index"):
        """æ”¯æŒç¼“å­˜çš„ FAISS index æž„å»º"""

        index_path = save_path + ".index"
        chunks_path = save_path + "_chunks.pkl"
        emb_path = save_path + "_emb.npy"

        # å¦‚æžœç¼“å­˜å­˜åœ¨ â†’ ç›´æŽ¥åŠ è½½
        if os.path.exists(index_path) and os.path.exists(chunks_path):
            # print(f"[INFO] Loading cached FAISS index from {save_path}")
            logging.info(f" Loading cached FAISS index from {save_path}")
            index = faiss.read_index(index_path)
            with open(chunks_path, "rb") as f:
                chunks = pickle.load(f)
            # embeddings = np.load(emb_path)
            return index, chunks


        logging.info(f" Building FAISS index for {len(chunks)} chunks...")
        embeddings = self.embedding_model.encode(
            chunks,
            batch_size=512,  # è°ƒå¤§
            convert_to_numpy=True,
            show_progress_bar=True,
            device="cuda" if torch.cuda.is_available() else "cpu"
        ).astype("float32")

        dim = embeddings.shape[1]
        index = faiss.IndexFlatL2(dim)
        index.add(embeddings)

        # ä¿å­˜ç¼“å­˜
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        faiss.write_index(index, index_path)
        with open(chunks_path, "wb") as f:
            pickle.dump(chunks, f)
        np.save(emb_path, embeddings)

        # print(f"[INFO] FAISS index saved to {save_path}")
        logging.info(f" FAISS index saved to {save_path}")
        return index, embeddings, chunks

    # æ£€ç´¢ç›¸ä¼¼çš„æ–‡æœ¬å—
    def retrieve_similar_chunks(self, query):
        """æ£€ç´¢å‰Kä¸ªæœ€ç›¸å…³çš„æ–‡æœ¬å—"""
        query_embedding = self.embedding_model.encode([query])
        query_embedding = np.array(query_embedding)
        distances, indices = self.index.search(np.array(query_embedding), self.args.k)
        return [self.chunks_search[i] for i in indices[0]if i >= 0]


    def get_content(self, query):  # query_list æ˜¯ä¸€ä¸ª JSON array
        """
        ä»Žæœ¬åœ°çŸ¥è¯†åº“ä¸­è¿›è¡Œ RAG æ£€ç´¢ã€‚
        query_list: list[str]ï¼Œä¾‹å¦‚ ["RoboCop film language", "English Language", "American English"]
        è¿”å›žï¼šåˆ† chunk çš„æ–‡æœ¬åˆ—è¡¨
        """
        all_chunks = []
        # å¦‚æžœ query æ˜¯å­—ç¬¦ä¸²ï¼Œå°±å°è¯•è§£æžæˆ JSON array
        if isinstance(query, str):
            try:
                query_list = json.loads(query)
            except json.JSONDecodeError:
                # å¦‚æžœä¸æ˜¯åˆæ³• JSONï¼Œå°±é€€åŒ–ä¸ºå•ä¸€å­—ç¬¦ä¸²æŸ¥è¯¢
                query_list = [query]
        elif isinstance(query, list):
            query_list = query
        else:
            raise ValueError(f"Unsupported query type: {type(query)}")

        logging.info(f"Searching queries: {query_list}")

        for q in query_list:
            logging.info(f"Searching for query: {q}")
            top_chunks = self.retrieve_similar_chunks(q)
            if top_chunks:
                all_chunks.extend(top_chunks)
            else:
                logging.info(f">>> No good Knowledge Graph Search Result was found for query: {q}")

        if not all_chunks:
            return None

        # åŽ»é‡
        all_chunks = list(set(all_chunks))

        # æ‹¼æŽ¥ä¸ºæ•´ä½“æ–‡æœ¬
        retrieved_text = " ".join(all_chunks)

        # åˆ‡åˆ†æˆæ›´å°çš„æ®µè½
        trunked_texts = self.chunk_text_by_sentence(retrieved_text, 1500)
        trunked_texts = [trunked_text.replace('\n', " ") for trunked_text in trunked_texts]

        return trunked_texts

    def relation_text(self, relation, align_text):
        if align_text=="True":
            return self.align_text[relation]
        else:
            relation_hierachy_list = relation.strip().replace('.',' ').split('/')
            final_string = ''
            for st in reversed(relation_hierachy_list):
                if st != "":
                    final_string += st + " of "
            return final_string

    def count_token(self, string):
        """Returns the number of tokens in a text string using tiktoken."""
        if string is None or string == "":
            return 0
        encoding = tiktoken.get_encoding(self.encoding_name)
        num_tokens = len(encoding.encode(string))
        return num_tokens


    def forward(self, relation, entity):
        self.LLM.reset_history()
        self.reset_history()

        ##å¾—åˆ°candidateçš„id
        ent_str = self.ent2text[entity]
        candidate_ids = self.all_candidate_answers['\t'.join([str(self.ent2id[entity]), str(self.rel2id[relation])])]

        ##å¾—åˆ°candidateçš„text
        for id in candidate_ids[:self.args.candidate_num]:
            self.candidate_answers.append(self.ent2text[self.id2ent[str(id)]])
        origin_candidates_text = self.serialize_candidate_answers()

        ##å¾—åˆ°demonstration_text
        question_text = ''
        if self.args.query == 'tail':
            question_text = self.generate_demonstration_text((ent_str, relation, ''))
        elif self.args.query == 'head':
            question_text = self.generate_demonstration_text(('', relation, ent_str))

        current_demon_response = self.LLM.get_response((question_text),"first_give_demonstration")

        ###true_demons: [[tpe_text,question,t1_text],[tpe_text,question,t2_text],...]
        true_demons = self.prompt_selector.true_candidate_v2(entity, relation, num=args.demon_per_step // 2)

        ####å¾—åˆ°åºåˆ—åŒ–ä¹‹åŽçš„promptä¸­çš„æ‰€æœ‰çœŸå®žä¸‰å…ƒç»„æ¡ˆä¾‹ç»„æˆçš„ä¸€æ®µtextï¼štrue_demon_textï¼Œæ³¨æ„è¿™é‡Œé¢éƒ½æ˜¯çœŸå®žç­”æ¡ˆçš„ï¼Œä¹Ÿå³åœ¨åºåˆ—åŒ–æ—¶t!= ''
        true_demon_text = self.serialize_demonstrations(true_demons)
        if true_demon_text != "None.":
            # è¯¥æ­¥éª¤æ”¾å…¥promptçš„æ˜¯å½“å‰queryçš„relation-entityåœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸­çš„ç­”æ¡ˆé›†ã€‚è¯¥æ­¥éª¤çš„ç›®çš„åº”è¯¥æ˜¯åœ¨æœ€å¼€å§‹ä¼˜å…ˆåœ¨promptä¸­æ”¾ä¸Žå½“å‰queryæœ€ç›¸å…³çš„ä¸‰å…ƒç»„ä¿¡æ¯ã€‚
            current_demon_response = self.LLM.get_response((true_demon_text), "analogy_demonstration")
        if self.LLM.token_num >= args.max_llm_input_tokens - 1000:
            self.LLM.history_messages.pop()
            self.LLM.history_messages.pop()
            self.LLM.history_contents.pop()
            self.LLM.history_contents.pop()


        ### RAT pipline
        logging.info(f"{datetime.now()} Obtaining Draft ...")
        ####llmæŽ’åº
        draft = self.LLM.get_response((question_text, origin_candidates_text), "init_draft")
        logging.info(f"{datetime.now()}  Returning Draft")
        logging.info(f"Draft: {draft}")

        logging.info(f"{datetime.now()}  Handling Draft ...")
        draft_paragraphs = self.split_draft(draft)
        logging.info(f"{datetime.now()}  The draft has been split into {len(draft_paragraphs)} parts")
        answer = ""

        for i, p in enumerate(draft_paragraphs):
            logging.info(f"{datetime.now()}  Revise the {i + 1}/{len(draft_paragraphs)}th part...")

            ###ä¸æ–­æºå¸¦å‰é¢çš„åŽ†å²ç­”æ¡ˆè¿›è¡Œä¿®æ­£
            answer = answer + '\n\n' + p
            ####æŸ¥çœ‹pæœ‰å‡ å¥è¯ï¼Œæˆ–è€…æ”¹æˆå°¤å…¶æ˜¯æœ€åŽä¸€æ®µè€Œä¸æ˜¯å°¤å…¶æ˜¯æœ€åŽä¸€æ®µè¯
            logging.info(f"{datetime.now()}  Generating Corresponding Query...")
            ###å¾—åˆ°ä¸Žè¯¥éƒ¨åˆ†å†…å®¹ç›¸å…³çš„query
            res = self.LLM.get_response((question_text, answer), "query_generation")
            if hasattr(res, "content"):
                    query = res.content
            elif isinstance(res, str):
                    query = res
            else:
                logging.warning(f"Unexpected response type: {type(res)}. Skipping...")
                continue

            if self.LLM.token_num >= self.args.max_llm_input_tokens:
                self.LLM.history_messages.pop()
                self.LLM.history_messages.pop()
                self.LLM.history_contents.pop()
                self.LLM.history_contents.pop()
                break
            logging.info(f">>> {i}/{len(draft_paragraphs)} Query: {query.replace(self.newline_char, ' ')}")  ##str.replace(old, new[, count])

            logging.info(f"{datetime.now()}  Obtaining Knowledge graph content...")
            ###æ ¹æ®queryæ¥searchåˆ°ç›¸å…³çš„page
            result = self.get_content(query)
            if not result:
                logging.info(f"{datetime.now()}  Skip the subsequent steps...")
                continue
            else:
                content = result
            for j, c in enumerate(content):
                if j > 3:
                    break
                logging.info(f"{datetime.now()}  Revise corresponding content according to local knowledge graph...[{j}/{min(len(content), 3)}]")

                res = self.LLM.get_response((question_text, answer, c), "revise")
                if not res:
                    logging.info(f"{datetime.now()}  Skip the subsequent steps...")
                    continue
                else:
                    answer = res
                logging.info(f"{datetime.now()}  Revise Finished [{j}/{min(len(content), 3)}]")
                if self.LLM.token_num >= self.args.max_llm_input_tokens-1000:
                # if self.LLM.token_num >= self.args.max_llm_input_tokens - query_token_num:
                    self.LLM.history_messages.pop()
                    self.LLM.history_messages.pop()
                    self.LLM.history_contents.pop()
                    self.LLM.history_contents.pop()
                    break
        final_response = self.LLM.get_response((origin_candidates_text,question_text,answer), "final_query_template")
        self.log.append(final_response)
        final_order = self.parse_result(final_response, entity, relation)
        self.log.append(final_order)
        used_tokens = self.LLM.token_num
        return final_order, self.LLM.history_contents, self.log, used_tokens

    def chunk_text_by_sentence(self, text, chunk_size=2048):
        sentences = re.split(r'(?<=[.!?ã€‚ï¼ï¼Ÿ])\s+', text)  # è‹±æ–‡ + ä¸­æ–‡ç¬¦å·
        chunked_text = []
        curr_chunk = []

        for sentence in sentences:
            if self.count_token(" ".join(curr_chunk)) + self.count_token(
                    sentence) + 2 <= chunk_size:
                curr_chunk.append(sentence)
            else:
                if curr_chunk:
                    chunked_text.append(". ".join(curr_chunk))
                curr_chunk = [sentence]

        if curr_chunk:
            chunked_text.append(". ".join(curr_chunk))
        return chunked_text


    def serialize_candidate_answers(self):
        """
        å°†å¤šä¸ªä¸‰å…ƒç»„æˆ–å®žä½“æ–‡æœ¬æ‹¼æŽ¥æˆå­—ç¬¦ä¸²ï¼Œç”¨äºŽæ’å…¥åˆ°æœ€ç»ˆ prompt æ¨¡æ¿ä¸­
        """
        candidiate_str = '[' + ','.join(self.candidate_answers) + ']'
        return candidiate_str

    def serialize_demonstrations(self, demon_triples):  ###å¾—åˆ°å°†æ‰€æœ‰ä¸‰å…ƒç»„è¿›è¡Œåºåˆ—åŒ–ä¹‹åŽçš„text
        """
        å°†å¤šä¸ªä¸‰å…ƒç»„æˆ–å®žä½“æ–‡æœ¬æ‹¼æŽ¥æˆå­—ç¬¦ä¸²ï¼Œç”¨äºŽæ’å…¥åˆ°æœ€ç»ˆ prompt æ¨¡æ¿ä¸­
        """
        demon_text = ""
        for tp in demon_triples:
            demon_text += self.generate_demonstration_text(tp) + '. '
        demon_text.strip()
        if demon_text == "": demon_text = "None."
        return demon_text

    def generate_demonstration_text(self, triple):
        """

       å°†ä¸‰å…ƒç»„æ ¼å¼åŒ–ä¸ºè‡ªç„¶è¯­è¨€ promptï¼ˆä»¥ mask è¡¨è¾¾ï¼‰
       ç”¨äºŽæž„å»º Prompt ä¸­çš„ few-shot ç¤ºä¾‹ï¼Œå¦‚ï¼š
       predict the tail entity [MASK] from the given (Barack Obama, place of birth, [MASK]) by completing the sentence
       "what is the place of birth of Barack Obama? The answer is ... "
       """
        h, r, t = triple
        demonstration_text = ""
        if self.args.query == 'tail':
            if self.args.align_text:
                demonstration_text = 'predict the tail entity [MASK] from the given ('
                demonstration_text += h + ', ' + self.relation_text(r, False)
                demonstration_text += ", [MASK]) by completing the sentence \""
                demonstration_text += (self.relation_text(r, True).replace("[H]", h).
                                       replace("[T]", "[the answer]") + '? The answer is \"')
                if t != '':
                    demonstration_text += ". The answer is " + t + ", so the [MASK] is " + t
            else:
                demonstration_text = 'predict the tail entity [MASK] from the given ('
                demonstration_text += h + ', ' + self.relation_text(r, False)
                demonstration_text += ", [MASK]) by completing the sentence \"what is the "
                demonstration_text += self.relation_text(r, False) + h + '? The answer is \"'
                if t != '':
                    demonstration_text += ". The answer is " + t + ", so the [MASK] is " + t
        elif self.args.query == 'head':
            if self.args.align_text:
                demonstration_text = 'predict the head entity [MASK] from the given ('
                demonstration_text += '[MASK]' + ', ' + self.relation_text(r, False)
                demonstration_text += ", " + t + ") by completing the sentence \""
                demonstration_text += self.relation_text(r, True).replace("[H]", "[the answer]").replace("[T]",
                                                                                                         t) + '? The answer is \"'
                if h != '':
                    demonstration_text += ". The answer is " + h + ", so the [MASK] is " + h
            else:
                demonstration_text = 'predict the head entity [MASK] from the given ('
                demonstration_text += '[MASK]' + ', ' + self.relation_text(r, False)
                demonstration_text += ", " + t + ") by completing the sentence \"" + t + " is the "
                demonstration_text += self.relation_text(r, False) + "what" + '? The answer is \"'
                if h != '':
                    demonstration_text += ". The answer is " + h + ", so the [MASK] is " + h
        return demonstration_text

    def parse_result(self, response, entity, relation):
        response = response.lower()
        candidate_answers = []
        ##å¾—åˆ°candidateçš„id
        candidate_ids = self.all_candidate_answers['\t'.join([str(self.ent2id[entity]), str(self.rel2id[relation])])]

        ##å¾—åˆ°candidateçš„text
        for id in candidate_ids[:self.args.candidate_num]:
            candidate_answers.append(self.ent2text[self.id2ent[str(id)]])

        if "the final order:" in response:
            final_order_raw = re.split("the final order:", response)[1].strip().strip('.').strip('\[').strip('\]')
            final_order_raw_list = final_order_raw.split(' | ')
            final_order_list = []
            for candidate in final_order_raw_list:
                if candidate not in final_order_list:
                    final_order_list.append(candidate)
            final_order = ' | '.join(final_order_list)
        else:
            final_order = ' | '.join(candidate_answers)

        return final_order

    def check_work_flow(self, response):
        if "no" in response.lower():
            return False
        return True

    def reset_history(self):
        self.log = []
        self.candidate_answers = []
        self.selected_demonstrations = []

    def load_all_candidate_answers(self):
        with open("/root/autodl-tmp/dataset/" + self.args.dataset + "/retriever_candidate_" + self.args.query + ".txt", 'r') as load_f:
            self.all_candidate_answers = json.load(load_f)  # æ¯ä¸ªé—®é¢˜æœ‰100ä¸ªcandidates

    def load_align_text(self):
        with open("/root/autodl-tmp/dataset/" + self.args.dataset + "/alignment/alignment_clean.txt", 'r') as load_f:
            self.align_text = json.load(load_f)

    def load_rel_txt_to_id(self):
        with open('/root/autodl-tmp/dataset/' + self.args.dataset + '/get_neighbor/relation2id.txt', 'r') as file:
            relation_lines = file.readlines()
            for line in relation_lines:
                _name, _id = line.strip().split("\t")
                self.rel2id[_name] = _id

    def load_ent_map_id(self):
        with open('/root/autodl-tmp/dataset/' + self.args.dataset + '/get_neighbor/entity2id.txt', 'r') as file:
            entity_lines = file.readlines()
            for line in entity_lines:
                _name, _id = line.strip().split("\t")
                self.ent2id[_name] = _id
                self.id2ent[_id] = _name

    def load_ent_to_text(self):
        with open('/root/autodl-tmp/dataset/' + self.args.dataset + '/entity2text.txt', 'r') as file:
            entity_lines = file.readlines()
            for line in entity_lines:
                ent, text = line.strip().split("\t")
                self.ent2text[ent] = text

    def split_draft(self, draft, split_char='###'):
        # å°†draftåˆ‡åˆ†ä¸ºå¤šä¸ªæ®µè½
        draft_paragraphs = [p.strip() for p in draft.split(split_char) if p.strip()]
        return draft_paragraphs


def main(args, all_data, idx, api_key):
    """
    åœ¨æµ‹è¯•é›†test.answer.txtä¸Šè¿è¡Œratæ¥å¾—åˆ°æ¯ä¸€ä¸ªæµ‹è¯•å®žä¾‹çš„é¢„æµ‹ç»“æžœï¼Œå¹¶å°†ç»“æžœä¿å­˜åœ¨outputæ–‡ä»¶ä¸­
    """
    openai.api_key = api_key
    if idx == -1:
        output_path = args.output_path
        chat_log_path = args.chat_log_path
    else:  # å¤šçº¿ç¨‹
        idx = "0" + str(idx) if idx < 10 else str(idx)  # 00 01 02 ... 29
        output_path = args.output_path + "_" + idx
        chat_log_path = args.chat_log_path + "_" + idx

    logging.info(f"Start PID {os.getpid()} and save to {output_path}")
    rat = RAT(args)

    count = 0
    valid_count = 0
    all_used_tokens = []

    with open(output_path, "w") as f:
        with open(chat_log_path, "w") as fclog:
            for sample in tqdm(all_data, total=len(all_data)):  ##all_dataï¼šæµ‹è¯•é›†test.tsv
                count += 1
                # sample:
                # {"ID": 2477,
                #   "HeadEntity": "/m/0299hs",
                #   "Answer": "/m/02h40lc",
                #   "Question": "/film/film/language"}
                try:
                    tpe = sample['HeadEntity'] if args.query == 'tail' else sample['Answer']
                    question = sample['Question']  ###relation

                    prediction, chat_history, record,used_tokens = rat.forward(question, tpe)  ###è¾“å…¥å…³ç³»å’Œå¤´å®žä½“ï¼ˆå°¾å®žä½“é¢„æµ‹ï¼‰æˆ–å°¾å®žä½“ï¼ˆé¢„æµ‹å¤´å®žä½“ï¼‰
                    valid_count += 1
                    all_used_tokens.append(used_tokens)

                except BadRequestError as e:  # å¯¹åº”æ—§ç‰ˆçš„ InvalidRequestError
                    print(e)
                    continue
                except OpenAIError as e:  # æ•èŽ·å…¶ä»– OpenAI ç›¸å…³é”™è¯¯
                    logging.exception(e)
                    continue
                except Exception as e:  # æ•èŽ·éž OpenAI ç›¸å…³çš„é”™è¯¯
                    logging.exception(e)
                    continue


                chat = str(sample["ID"]) + "\n" + "\n******\n".join(chat_history) + "\nAnswers: " + str(
                    sample['Answer']) + "\n------------------------------------------\n"
                fclog.write(chat)

                sample["Prediction"] = prediction
                f.write(json.dumps(sample) + "\n")

    avg_tokens = sum(all_used_tokens) / len(all_used_tokens)
    print(f"Average tokens per instance: {avg_tokens:.2f}")

    logging.info("---------------PID %d end with %d/%d samples--------------" % (os.getpid(), valid_count, count))


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset', default="fb15k-237")
    parser.add_argument('--candidate_num', default=50, type=int)
    parser.add_argument('--output_path', default="./outputs/fb15k-237/output_tail.txt")#fb15k-237
    parser.add_argument('--chat_log_path', default="./outputs/fb15k-237/chat_tail.txt")
    parser.add_argument('--query', default="tail")
    parser.add_argument('--model_path', default=None)

    parser.add_argument('--debug', action="store_true")
    parser.add_argument('--debug_online', action="store_true")
    parser.add_argument('--align_text', default="False")

    parser.add_argument('--max_tokens', default=1500, type=int, help='max-token')
    parser.add_argument('--prompt_path', default="./prompts/icl_rat.json")
    parser.add_argument('--prompt_name', default="chat", )
    parser.add_argument('--bagging_type', default="llm", )
    parser.add_argument('--overwrite', action="store_true")
    parser.add_argument('--device', default=0, help='the gpu device')

    parser.add_argument('--api_key', default="sk-JMU1FWOBnZra7gVz1DRV8MlmNT7yCVr4VRB4PP539c5qRS80", type=str)
    parser.add_argument('--demon_per_step', default=10)
    parser.add_argument('--eff_demon_step', default=10)
    parser.add_argument('--max_demon_step', default=10)
    parser.add_argument('--max_llm_input_tokens', default=5000, type=int)
    parser.add_argument('--num_process', default=1, type=int, help='the number of multi-process')

    parser.add_argument('--supporting_file_path', default="/root/autodl-tmp/dataset/fb15k-237/description/supporting_file.txt")
    parser.add_argument('--chunk_size', default=500)
    parser.add_argument('--chunk_overlap', default=100)
    parser.add_argument('--k', default=1)
    parser.add_argument("--entity_file", default="/root/autodl-tmp/dataset/fb15k-237/entity2text.txt")
    parser.add_argument("--filter_file", default="/root/autodl-tmp/dataset/fb15k-237/filter_head.txt")

    args = parser.parse_args()
    args.output_path = './outputs/' + args.dataset + '/output_' + args.query + '_rat-context.txt'
    args.chat_log_path = './outputs/' + args.dataset + '/chat_' + args.query + '_rat-context.txt'
    logging.info("Start querying the LLM.")
    return args
def merge_outputs(args):
    """
    å°†å¤šä¸ªå­è¿›ç¨‹çš„ç»“æžœæ–‡ä»¶åˆå¹¶æˆæœ€ç»ˆæ–‡ä»¶
    """
    final_output = args.output_path
    final_chatlog = args.chat_log_path

    with open(final_output, "w") as fout, open(final_chatlog, "w") as fchat:
        for fname in sorted(os.listdir(os.path.dirname(final_output))):
            if fname.startswith(os.path.basename(final_output)):
                with open(os.path.join(os.path.dirname(final_output), fname)) as f:
                    for line in f:
                        fout.write(line)
            if fname.startswith(os.path.basename(final_chatlog)):
                with open(os.path.join(os.path.dirname(final_chatlog), fname)) as f:
                    for line in f:
                        fchat.write(line)

    logging.info(f"âœ… åˆå¹¶å®Œæˆï¼š{final_output}, {final_chatlog}")


if __name__ == '__main__':
    args = parse_args()
    random.seed(42)

    if not args.api_key.startswith("sk-"):
        with open(args.api_key, "r") as f:
            all_keys = f.readlines()
            all_keys = [line.strip('\n') for line in all_keys]
            assert len(all_keys) == args.num_process, (len(all_keys), args.num_process)

    with open("/root/autodl-tmp/dataset/" + args.dataset + "/select_sample.txt", 'r') as load_f:  ###åŠ è½½testçš„ä¸‰å…ƒç»„
        test_triplet = json.load(load_f)

    test_triplet = test_triplet[:6]

    logging.info("Totally %d test examples." % len(test_triplet))


    if args.debug_online:
        test_triplet = test_triplet[0:2 * args.num_process]

    if args.num_process == 1:
        main(args, test_triplet, idx=-1, api_key=args.api_key)
    else:
        num_each_split = int(len(test_triplet) / args.num_process)
        p = mp.Pool(args.num_process)
        for idx in range(args.num_process):
            start = idx * num_each_split
            if idx == args.num_process - 1:
                end = max((idx + 1) * num_each_split, len(test_triplet))
            else:
                end = (idx + 1) * num_each_split
            split_data = test_triplet[start:end]

            if args.api_key.startswith("sk-"):
                api_key = args.api_key
            else:
                api_key = all_keys[idx]

            try:
                p.apply_async(main, args=(args, split_data, idx, api_key))
            except Exception as e:
                logging.exception(e)

        p.close()
        p.join()
        merge_outputs(args)
        logging.info("All of the child processes over!")




    # predictions = load_predictions(args.output_path)
    # entity_text = load_ent_to_text(args.entity_file)
    # filter_tail = load_filter_tail(args.filter_file)
    #
    # metrics = evaluate_ranking(predictions, entity_text, filter_tail)
    # print("=== Evaluation Results ===")
    # for k, v in metrics.items():
    #     print(f"{k}: {v:.4f}")


